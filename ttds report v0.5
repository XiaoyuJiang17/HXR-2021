preprocessing:

First step,I implement tokenisation, stopword removing and stemming in order. 其实他们之间是相关关联并互相作用于各自的，

比如tokenisation:寻找文本中的特征标记,通过特殊字符匹配包括下划线、大小写字母和阿拉伯数字，并将其替换为空格后写入文件，因此去掉了下划线、大小写字母和阿拉伯数字的剩余字符都视作特征标记，然后将其转为小写，并从完整文本中抽取所有非停词词干， 寻找文本中的特征字符，即去掉所有下划线、大小写字母和阿拉伯数字后的列表。全部转小写。选取所有非停词后根据所有非停词计算词干。如果单词长度小于等于2，则返回本身，否则返回单词的原始形态，最后返回stem_list.

XML:

meanhile,i have set up a gobal varible is sample = false =, 方便切换测试集， which appear several times at my code,so it may let my code more clearly and easy to change.  
将非标准XML文件变为XML文件。实质是在文件头和尾加入 XML DOM 节点,通过def xml_sample 读取指定的 XML 文件，并返回它的根节点。
def xml_all_text 提取所有词干。
sample 和 trec 的唯一区别是，sample 只有 text 属性，而 trec 有 headline 和 text 属性，从 trec 提取需要将 headline 和 text 合并。需要注意的是，不能删除“_”。所有文本都转换成小写。然后对修改后的文本应用split（）以获取所有标记。处理停止字列表中出现的所有标记。
在上述所有过程之后，使用词干分析来获取每个令牌的词干





3倒排索引


The key of the dictionary is the term after pre-processing. And the value of the dictionary is a dictionary with the document number as the key and the position
which the term appears as the value. It may not be the fastest way for storing the positional inverted index, but it’s easy to implement.

我使用celementtree将标题和文本从XML解析到列表中。然后将反向索引存储到一个叫做术语dic的字典字典中。字典的关键字是预处理后的术语。字典的值是一个字典，文档编号为键，术语出现的位置是值。它可能不是存储位置反向索引的最快方法，但它很容易实现。

它只需要对所有经过预处理的文本进行一次迭代。每次，将当前词干的位置附加到索引字典。因为预处理的文本存储在元组列表中。
因此，当我迭代每个stem时，很容易获得文档id stem和词干位置（在Python中使用列表索引）。
即使我在for循环中使用append（）,然后使用Python内置函数file write以所需格式将反向索引字典输出到index.txt。索引后，每次搜索时，只需要从index.txt加载索引。

Query部分

在本课程中，只给出了两种类型的搜索，布尔搜索和分级检索。因此，我编写代码以通过文件名来区分搜索类型，使用split（'..'）[1]来获取类型。布尔搜索比其他搜索稍微复杂一些。
首先，我为单个术语或短语查询创建了几个函数。也就是说，将查询拆分为“AND”或“or”，然后使用这些函数执行单个查询。在获得单个查询结果后，决定是否需要将其与文档id集之间的差异集（不包含则是必要的）。然后使用并集或交集合并单个查询结果。如果查询中没有“AND”或“or”，只需按以下步骤执行单个查询
提及（还需要检查术语是否包含“NOT”前缀）。
给定一个布尔搜索txt文件，读取每一行。然后获取查询id，当前行的其余部分就是查询。如果“AND”或“or”在查询中，请使用前面提到的方法
上面的步骤可以得到结果。如果不是，则直接使用函数boolean_query（）获取单个查询结果。首先，检查并移除（如果需要）NOT。然后检查查询是否以“#”开头（applied.strip（）在检查之前，确保删除所有空格）。如果是，则获取短语搜索距离（使用拆分函数，按“（”）和两个词拆分），然后进行近似搜索。如果不是，则检查查询是否以“.”开头，也以“.”结尾。如果是，则进行短语搜索。如果不是，则进行单词查询。在近似搜索中，仅迭代
这两个术语的反向索引都包含的文档。使用索引来迭代当前文档中的位置列表。在每次迭代中，只需比较差异以确定需要向前移动的索引。短语搜索也是如此。
当给定一个排序的搜索txt文件时，我会像布尔搜索文件一样获取文档id和查询。从查询中的术语进行迭代，获取文档频率，然后我使用倒排法迭代每个文档中包含当前术语的每个术语位置列表。
索引。
只需使用len（）获得列表长度。这样我就可以获得术语频率和文档频率。
在每个术语的每次迭代中，我使用字典存储每个文档的累积TF-IDF（在每个术语的每次迭代中，只需更新文档的TF-IDF，其中至少包含一个当前术语）TF-IDF的具体方程是其中q点一个项，d点一个文档。
值得一提的是，这两种类型查询中的每个术语都需要转换为小写形式。
然后使用小写形式获取词干形式。因为这是我对集合所做的。查询术语需要进行相同的预处理。
最后将查询结果以所需格式输出到不同的文件中，函数名为output_query（），如附录所示。

tfidf
   计算所有词的 TF_IDF 权重，并按从大到小排序。
    Term Frequency(TF) = 1 + lg (某个词出现的数量 / 总词数)
    Inverse Document Frequency(IDF) = lg(总文件数 / 出现某个词的文件 
    Query
    define a line, to judge the number of lines > 150
    
    
    
    感想：
    This coursework is just a rough implementation of simple query of IR but I’ve learnt a lot from it. The implementation of each part is not very complicated but we have to take efficiency and effectiveness into consideration. After many times of debugging, I have a clear idea of where the bugs are most likely
to appear, which is a valuable experience for future work. Also, I’ve practiced the essential programming skills in Python and gotten to learn some knowledge
about algorithms and data structures. When building the system, many advanced topics come to my mind: what if two or more “and” and “or” appears in
the query? What if the proximity search contains more than two terms? How to measure whether the user likes the result? I find these topics really interesting
and I’m eager to go deeper on them later on.





