
dropout:

Dropout可以看做是一种集成模型的思想，在每个step中，会将网络的隐层节点以概率 [公式] 置0。Dropout和传统的bagging方法主要有以下两个方面不同：

Dropout的每个子模型的权值是共享的；在训练的每个step中，Dropout每次会使用不同的样本子集训练不同的子网络。

这样在每个step中都会有不同的节点参与训练，减轻了节点之间的耦合性。在测试时，使用的是整个网络的所有节点，只是节点的输出值要乘以Dropout的概率 [公式] 





Maxout Network:

why choose maxout:作者认为，与其像Dropout这种毫无选择的平均，不如有条件的选择节点来生成网络.

在Maxout网络中，W是一个三维矩阵，矩阵的维度是 [公式] ，其中 [公式] 表示Maxout网络的通道数，是Maxout网络唯一的参数

Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个 maxout 节点就可以拟合任意的凸函数了（相减），前提是”隐隐含层”节点的个数可以任意多.

这样 Maxout 神经元就拥有 ReLU 单元的所有优点（线性和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和 ReLU 对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增

strength:
Maxout的拟合能力非常强，可以拟合任意的凸函数。Maxout具有ReLU的所有优点，线性、不饱和性。同时没有ReLU的一些缺点。如：神经元的死亡。


weakness:
在keras2.0之前的版本中，我们可以找到Maxout网络的实现，其核心代码只有一行。

output = K.max(K.dot(X, self.W) + self.b, axis=1)
从上面的激活函数公式中可以看出，每个神经元中有两组(w,b)参数，那么参数量就增加了一倍，这就导致了整体参数的数量激增。
Maxout网络存在的最大的一个问题是，网络的参数是传统网络的 [公式] 倍， [公式] 倍的参数数量并没有带来其等价的精度提升，现基本已被工业界淘汰


#存储数据代码,保存结果，输出到table上

val_acc_dropout_1 = stats_list_dropout[0][1:,keys_list_dropout[0][['acc(valid)']][-1]
val_acc_dropout_2 = stats_list_dropout[1][1:,keys_list_dropout[1][['acc(valid)']][-1]
val_acc_dropout_3 = stats_list_dropout[2][1:,keys_list_dropout[2][['acc(valid)']][-1]
val_acc_dropout_4 = stats_list_dropout[3][1:,keys_list_dropout[3][['acc(valid)']][-1]
val_acc_dropout_5 = stats_list_dropout[4][1:,keys_list_dropout[4][['acc(valid)']][-1]



generalization_gap_dropout_1 = stats_list_dropout[0][1:,keys_list_dropout[0][['error(valid)']][-1]
generalization_gap_dropout_2 = stats_list_dropout[1][1:,keys_list_dropout[1][['error(valid)']][-1]
generalization_gap_dropout_3 = stats_list_dropout[2][1:,keys_list_dropout[2][['error(valid)']][-1]
generalization_gap_dropout_4 = stats_list_dropout[3][1:,keys_list_dropout[3][['error(valid)']][-1]
generalization_gap_dropout_5 = stats_list_dropout[4][1:,keys_list_dropout[4][['error(valid)']][-1]


print('val_acc_dropout_1',val_acc_dropout_1)
print('val_acc_dropout_2',val_acc_dropout_2)

print('generalization_gap_dropout_1',generalization_gap_dropout_1)
print('generalization_gap_dropout_2',generalization_gap_dropout_2)


CW-Question
Q1: the neural network is so flexible that fit very closely to training data
（特别是学习到了噪声和异常数据）.but perform bad on test data.

Q2: the problem of overfitting will be more serious.

Q3: both approaches have significant contribution in releasing the severity of overfitting problems, 
and the combination of L1 and dropout gives the best performance.

Q4: overfitting is a pervasive issue exists in models with various width and depth, 
but various approaches have been showed effectiveness, such as dropout, L1/L2 regularization and Maxout.

Q5: the model has great performance on training dataset, while badly performed on unseen test dataset, 
which means the model is overtraining, fitted too closely to training dataset.

Q6: Overfitting occurs when our model is too flexible and the training dataset is relatively small, 
thus the model learned too many unnecessary details during training. One can identify this happen when comparing its performance on training and test dataset. 
One important sign of overfitting is the model performance on training dataset is much better than on test dataset.

Q7: the model accuracy by epochs on both training and validation set is shown. 
The accuracy on both training and validation datasets are increasing quickly during first 10 epochs, while after that, 
model accuracy continues increasing on training set but starts slow decreasing on validation set and the generalization gap in terms of accuracy increases during this time. Finally, after 100 epochs, there are a huge gap between accuracy on training and validation set, which are around 0.94 and 0.81 respectively. In figure 1b, We have opposite trend with cross-entropy error as the indicator. The tuning point is still at about 10th epoch, both training error and validation error decrease in rapid pace during first 10 epochs. Training error decreases continuously, but with slower speed, to less than 0.2 while validation error begin to take off, reaching 1.2 after 100 epochs

Q8:  for all three settings, the models perform much better on training set than validation set, 
in terms of both accuracy and generalization gap, 
which indicating all three models face serious overfitting problems. 
In addition, we observe that 128 neurons model having best performance in training set but worst in validation set, 
which means its overfitting issue is the most,
while gap between performance with 32 neurons model is the smallest,
meaning its overfitting problem is less severe. 
Generally, we find overfitting become increasingly serious as number of neurons increase. 

Q9: From our observation above, we find the width affects the performance results in a consistent way. 
This is as what we expected from the model flexibility point of view. 
The model with more neurons has larger flexibility, which is prone to become overfitting.

Q10: all models with three different depth are overfitted. 
Increasing the number of layers will lead to severer overfitting. 
The 3 hidden layers model has largest validation error and one of the lowest validation accuracies, 
while this is just contrary on training set,indicating this is the most overfitted model. 
The 1 hidden layer has less serious overfitting and much smaller gap in terms of training and validation performance.

Q11: From our observation above, 
we also find the depth affects the performance results in a consistent way which is similar to the width. 
This is also explainable by analyzing model flexibility.
The model with more hidden layers definitely has more neurons, resulting larger complexity and flexibility, 

Q12: From experiments above,
we find that increasing width or depth of neuron network both get model performance improved on training set 
but decreased on validation set. 
They both lead to overfitting. 
This can be understood because increasing both width and depth will lead to increase of model complexity(flexibility), resulting to overfitting. 

Q13: A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. 
Lasso Regression (Least Absolute Shrinkage and Selection Operator) adds “absolute value of magnitude” of coefficient as penalty term to the loss function.
Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function. Here the highlighted part represents L2 regularization element.
The idea of L1/L2 regularization is adding a complexity term to error function, 
so that the network function gets smoother after training. L1 regularization corresponds to adding a term based on summing the absolute values of the weights to the error: 
Similarly, L2 is defined as half of the sum of square of the weight. 
 
In practice, these are hyperparameter beta needed, which controls the relative importance of complexity term and data term.

Q14: Training error function with L1/L2 regularization term will discourage the model coefficients become huge which stops overfitting to training data. L1 and L2 regularization differ at several points: firstly, L1 and L2 have different rate of enforcing weights shrinking to zero. 
Thus, Secondly, generally speaking, L1 has the property of shrinking some of weight to 0 while leaving a few large important weights 
Here, if lambda is zero then you can imagine we get back OLS. However, if lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue.

Again, if lambda is zero then we will get back OLS whereas very large value will make coefficients zero hence it will under-fit

The key difference between these techniques is that Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.

Traditional methods like cross-validation, stepwise regression to handle overfitting and perform feature selection work well with a small set of features but these techniques are a great alternative when we are dealing with a large set of features.


Q15:实验detail

Q16. it has not previously been showed how model averaging in deep architectures performed. In addition, 
the author argued that instead of getting slight performance enhancement for applying Dropout to various models, 
its performance can be maximized when directly creating architectures suitable for utilizing dropout as model averaging technique. 

Q17. Maxout model benefits most from dropout, and highly compatible with it as model averaging techniques. 
The combination of these two yields state of art performance on four benchmark datasets. The author argued that the first reason is that dropout encourages Maxout units to get larger linear region, 
making network to learn model averaging techniques more efficiently. Experiment results shows that the dropout is indeed doing model averaging as expected with the assistance of Maxout,
and performed better than using Tanh activation. Secondly, maxout also improves dropout during training phrase. The author demonstrated that maxout model is easier to optimize than Relu when with dropout.

Q18. The general performance of proposed Maxout model is evaluated on four benchmark datasets, detailed settings including hyperparameter,
training epochs are also given.
In each dataset, designed model with maxout layers is tested, which obtains the best result compared to other state of art methods. 
Generally, the experiments on four benchmark datasets provide convincing evidence for the effectiveness of maxout network.

Q19
In this report, we explore the overfifitting problem of neural networks in depth. We fifind that the generalization ability of the model decreases, i.e., overfifitting, caused by less data, too many features, and repeated training. We obtained that 
increasing the number of hidden units and increasing the number of layers deepens the overfifitting degree. Therefore, we used Dropout and Regularization methods to improve the network architecture. 
These methods led to a reduction in the number of hidden features during training, but the training effect and accuracy did not change much. We also explored the combination of optimization methods and the effect of different hyperparameters on the overfifitting optimization. Finally, we observe the test performance of the best model which is with "dropout+L2, Dropout probability=0.65, Penalty coeficient=1e-4 with 3 hidden layers and 128 hidden units with ReLU activationfunctions".
Finally, we briefly review Maxout method, discuss its’ internal mechanism， strengths and weaknesses. After read this paper, a future research direction 未来方向猜测。











