
dropout:

Dropout可以看做是一种集成模型的思想，在每个step中，会将网络的隐层节点以概率 [公式] 置0。Dropout和传统的bagging方法主要有以下两个方面不同：

Dropout的每个子模型的权值是共享的；在训练的每个step中，Dropout每次会使用不同的样本子集训练不同的子网络。

这样在每个step中都会有不同的节点参与训练，减轻了节点之间的耦合性。在测试时，使用的是整个网络的所有节点，只是节点的输出值要乘以Dropout的概率 [公式] 





Maxout Network:

Maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。作者从数学的角度上也证明了这个结论，即只需2个 maxout 节点就可以拟合任意的凸函数了（相减），前提是”隐隐含层”节点的个数可以任意多.

这样 Maxout 神经元就拥有 ReLU 单元的所有优点（线性和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和 ReLU 对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增

strength:
Maxout的拟合能力非常强，可以拟合任意的凸函数。Maxout具有ReLU的所有优点，线性、不饱和性。同时没有ReLU的一些缺点。如：神经元的死亡。


weakness:
在keras2.0之前的版本中，我们可以找到Maxout网络的实现，其核心代码只有一行。

output = K.max(K.dot(X, self.W) + self.b, axis=1)
从上面的激活函数公式中可以看出，每个神经元中有两组(w,b)参数，那么参数量就增加了一倍，这就导致了整体参数的数量激增。
Maxout网络存在的最大的一个问题是，网络的参数是传统网络的 [公式] 倍， [公式] 倍的参数数量并没有带来其等价的精度提升，现基本已被工业界淘汰
